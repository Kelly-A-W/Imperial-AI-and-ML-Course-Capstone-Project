{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Required Capstone Component 12.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import any necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C, Matern\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "# load initial inputs\n",
        "x_1 = np.load(\"initial_data/function_1/initial_inputs.npy\")\n",
        "x_2 = np.load(\"initial_data/function_2/initial_inputs.npy\")\n",
        "x_3 = np.load(\"initial_data/function_3/initial_inputs.npy\")\n",
        "x_4 = np.load(\"initial_data/function_4/initial_inputs.npy\")\n",
        "x_5 = np.load(\"initial_data/function_5/initial_inputs.npy\")\n",
        "x_6 = np.load(\"initial_data/function_6/initial_inputs.npy\")\n",
        "x_7 = np.load(\"initial_data/function_7/initial_inputs.npy\")\n",
        "x_8 = np.load(\"initial_data/function_8/initial_inputs.npy\")\n",
        "\n",
        "# load initial outputs\n",
        "y_1 = np.load(\"initial_data/function_1/initial_outputs.npy\")\n",
        "y_2 = np.load(\"initial_data/function_2/initial_outputs.npy\")\n",
        "y_3 = np.load(\"initial_data/function_3/initial_outputs.npy\")\n",
        "y_4 = np.load(\"initial_data/function_4/initial_outputs.npy\")\n",
        "y_5 = np.load(\"initial_data/function_5/initial_outputs.npy\")\n",
        "y_6 = np.load(\"initial_data/function_6/initial_outputs.npy\")\n",
        "y_7 = np.load(\"initial_data/function_7/initial_outputs.npy\")\n",
        "y_8 = np.load(\"initial_data/function_8/initial_outputs.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1:\n",
        "\n",
        "A function with a 2D input and 1D output. \n",
        "\n",
        "*Application: Detect likely contamination sources in a two-dimensional area, such as a radiation field, where only proximity yields a non-zero reading. The system uses Bayesian optimisation to tune detection parameters and reliably identify both strong and weak sources.*\n",
        "\n",
        "I will use a Matern kernel because it is a generalisation of RBF and allows you to control for smoothness; and because the application is spatial, and the Matern kernel originated from spatial statistics. The Matern function has two parameters: length scale and $\\nu$. Length scale is the correlation decay parameter, and $\\nu$ controls the smoothness. \n",
        "\n",
        "The larger the length scale, the more correlated far away points are, while smaller values of length scales means the correlation between points decays faster with distance. Because I will not be doing much tuning of the parameters of my kernel, I will use the average distance between points as my length scale. \n",
        "\n",
        "$\\nu = 1.5$ corresponds to once differentiable functions, and $\\nu = 2.5$ corresponds to twice differentiable functions. As $\\nu$ tends towards infinity, the kernel becomes equivalent to the RBF kernel. So, the smaller $\\nu$, the less smooth the function.\n",
        "\n",
        "Source: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.31940389 0.76295937]\n",
            " [0.57432921 0.8798981 ]\n",
            " [0.73102363 0.73299988]\n",
            " [0.84035342 0.26473161]\n",
            " [0.65011406 0.68152635]\n",
            " [0.41043714 0.1475543 ]\n",
            " [0.31269116 0.07872278]\n",
            " [0.68341817 0.86105746]\n",
            " [0.08250725 0.40348751]\n",
            " [0.88388983 0.58225397]]\n",
            "[ 1.32267704e-079  1.03307824e-046  7.71087511e-016  3.34177101e-124\n",
            " -3.60606264e-003 -2.15924904e-054 -2.08909327e-091  2.53500115e-040\n",
            "  3.60677119e-081  6.22985647e-048]\n",
            "0.4435554854300381\n",
            "0.30556889047452995\n"
          ]
        }
      ],
      "source": [
        "#look at the data\n",
        "print(x_1)\n",
        "print(y_1)\n",
        "\n",
        "# look at average distance between points\n",
        "n = len(x_1[0])\n",
        "out_1 = np.sum(x_1[0] * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)\n",
        "out_2= np.sum(x_1[1] * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)\n",
        "print(abs(out_1))\n",
        "print(abs(out_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7.85415802e-05 4.97632277e-05 2.30928503e-05 ... 5.28156602e-03\n",
            " 5.33857113e-03 5.39323867e-03]\n",
            "[0.00631399 0.00607142 0.00583362 ... 0.00645337 0.00668342 0.00691831]\n",
            "[0. 1.]\n"
          ]
        }
      ],
      "source": [
        "# GP assumption\n",
        "noise_assumption = 1e-10\n",
        "\n",
        "# kernel parameters\n",
        "length_scale = [0.1, 0.1]\n",
        "nu = 1.5\n",
        "\n",
        "# set up evaluation grid:\n",
        "x_11 = np.linspace(0, 1, 100)\n",
        "x_12 = np.linspace(0, 1, 100)\n",
        "x_11, x_12 = np.meshgrid(x_11, x_12)\n",
        "x_1_grid = np.column_stack([x_11.ravel(), x_12.ravel()])\n",
        "\n",
        "# Define and fit GP\n",
        "kernel = Matern(length_scale = length_scale, nu = nu, length_scale_bounds=(1e-2, 1e5))\n",
        "model = GaussianProcessRegressor(kernel = kernel, alpha = noise_assumption)\n",
        "model.fit(np.array(x_1), np.array(y_1))\n",
        "\n",
        "# Get the predicted (posterior) mean and standard deviation for each grid point\n",
        "post_mean, post_std = model.predict(x_1_grid, return_std=True)\n",
        "print(post_mean)\n",
        "print(post_std)\n",
        "\n",
        "# Calculate the UCB aquisition function\n",
        "kappa = 0.0001\n",
        "UCB = post_mean + kappa * post_std\n",
        "\n",
        "# Get the next query point\n",
        "max_idx_1 = np.argmax(UCB)  \n",
        "next_point_1 = x_1_grid[max_idx_1] \n",
        "print(next_point_1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 2:\n",
        "\n",
        "A function with a 2D input and 1D output. \n",
        "\n",
        "*Application: Imagine a black box, or a mystery ML model, that takes two numbers as input and returns a log-likelihood score. Your goal is to maximise that score, but each output is noisy, and depending on where you start, you might get stuck in a local optimum.* \n",
        "\n",
        "*To tackle this, you use Bayesian optimisation, which selects the next inputs based on what it has learned so far. It balances exploration with exploitation, making it well suited to noisy outputs and complex functions with many local peaks.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 3:\n",
        "\n",
        "A function with a 3D input and 1D output. \n",
        "\n",
        "*Application: You’re working on a drug discovery project, testing combinations of three compounds to create a new medicine.*\n",
        "\n",
        "*Each experiment is stored in initial_inputs.npy as a 3D array, where each row lists the amounts of the three compounds used. After each experiment, you record the number of adverse reactions, stored in initial_outputs.npy as a 1D array.*\n",
        "\n",
        "*Your goal is to minimise side effects; in this competition, it is framed as maximisation by optimising a transformed output (e.g. the negative of side effects).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 4:\n",
        "\n",
        "A function with a 4D input and a 1D output.\n",
        "\n",
        "*Application: Address the challenge of optimally placing products across warehouses for a business with high online sales, where accurate calculations are costly and only feasible biweekly. To speed up decision-making, an ML model approximates these results within hours. The model has four hyperparameters to tune, and its output reflects the difference from the expensive baseline. Because the system is dynamic and full of local optima, it requires careful tuning and robust validation to find reliable, near-optimal solutions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 5:\n",
        "\n",
        "A function with a 4D input and a 1D output.\n",
        "\n",
        "*Application: You’re tasked with optimising a four-variable black-box function that represents the yield of a chemical process in a factory. The function is typically unimodal, with a single peak where yield is maximised.*\n",
        "\n",
        "*Your goal is to find the optimal combination of chemical inputs that delivers the highest possible yield, using systematic exploration and optimisation methods.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 6:\n",
        "\n",
        "A function with a 5D input and a 1D output.\n",
        "\n",
        "*Application: You’re optimising a cake recipe using a black-box function with five ingredient inputs, for example flour, sugar, eggs, butter and milk. Each recipe is evaluated with a combined score based on flavour, consistency, calories, waste and cost, where each factor contributes negative points as judged by an expert taster. This means the total score is negative by design.*\n",
        "\n",
        "*To frame this as a maximisation problem, your goal is to bring that score as close to zero as possible or, equivalently, to maximise the negative of the total sum.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 7:\n",
        "\n",
        "A function with a 6D input and a 1D output.\n",
        "\n",
        "*Application: You’re tasked with optimising an ML model by tuning six hyperparameters, for example learning rate, regularisation strength or number of hidden layers. The function you’re maximising is the model’s performance score (such as accuracy or F1), but since the relationship between inputs and output isn’t known, it’s treated as a black-box function.*\n",
        "\n",
        "*Because this is a commonly used model, you might benefit from researching best practices or literature to guide your initial search space. Your goal is to find the combination of hyperparameters that yields the highest possible performance.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 8:\n",
        "\n",
        "A function with a 8D input and a 1D output.\n",
        "\n",
        "*Application: You’re optimising an eight-dimensional black-box function, where each of the eight input parameters affects the output, but the internal mechanics are unknown.*\n",
        "\n",
        "*Your objective is to find the parameter combination that maximises the function’s output, such as performance, efficiency or validation accuracy. Because the function is high-dimensional and likely complex, global optimisation is hard, so identifying strong local maxima is often a practical strategy.*\n",
        "\n",
        "*For example, imagine you’re tuning an ML model with eight hyperparameters: learning rate, batch size, number of layers, dropout rate, regularisation strength, activation function (numerically encoded), optimiser type (encoded) and initial weight range. Each input set returns a single validation accuracy score between 0 and 1. Your goal is to maximise this score.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
